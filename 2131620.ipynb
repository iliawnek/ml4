{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries. Parts of the code are adapted from code from [Simon Rogers' FCML notebooks](https://github.com/sdrogers/fcmlcode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data set into NumPy arrays and define global constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "T = []\n",
    "feature_names = []\n",
    "\n",
    "with open('epi_stroma_data.tsv', 'rb') as tsv:\n",
    "    i = 0\n",
    "    for line in csv.reader(tsv, delimiter='\\t'):\n",
    "        if i != 0:\n",
    "            X.append([float(x) for x in line[1:]])\n",
    "            T.append([int(line[0])])\n",
    "        else:\n",
    "            feature_names = line\n",
    "        i += 1\n",
    "\n",
    "X = np.array(X)\n",
    "T = np.array(T)\n",
    "\n",
    "data_point_count = X.shape[0]\n",
    "feature_count = X.shape[1]\n",
    "fold_count = 23\n",
    "fold_size = data_point_count / fold_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement KNN and perform cross-validation to find the optimal value of K according to the minimisation of 0/1 loss. Also calculate the Matthews correlation coefficient for each value of K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def classify_knn(X, T, x_new, K):\n",
    "    # compute distance-class mappings\n",
    "    distances = []\n",
    "    for i, x in enumerate(X):\n",
    "        distances.append((np.linalg.norm(x_new - X[i]), T[i][0]))\n",
    "\n",
    "    # find most popular class in K-nearest neighbours\n",
    "    distances = sorted(distances, key=lambda d: d[0])\n",
    "    epi_votes = 0\n",
    "    stroma_votes = 0\n",
    "    for k in range(K):\n",
    "        if distances[k][1] == 1:\n",
    "            epi_votes += 1\n",
    "        else:\n",
    "            stroma_votes += 1\n",
    "    if epi_votes >= stroma_votes:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# Only perform cross-validation for K = 200 for demonstration/time reasons.\n",
    "# Change these values to evaluate for more than just K = 200.\n",
    "min_K = 200\n",
    "max_K = 200\n",
    "step_K = 10\n",
    "\n",
    "for K in range(min_K, max_K + 1, step_K):\n",
    "    true_epi = 0\n",
    "    true_stroma = 0\n",
    "    false_epi = 0\n",
    "    false_stroma = 0\n",
    "\n",
    "    for fold in range(fold_count):\n",
    "        print 'K = %d, fold = %d' % (K, fold + 1)\n",
    "\n",
    "        # divide folds into validation and training data\n",
    "        lower = fold_size * fold\n",
    "        upper = fold_size * (fold + 1)\n",
    "        X_fold = X[lower:upper]\n",
    "        X_train = np.delete(X, np.arange(lower, upper, 1), 0)\n",
    "        T_fold = T[lower:upper]\n",
    "        T_train = np.delete(T, np.arange(lower, upper, 1), 0)\n",
    "\n",
    "        # perform classification and quantify results\n",
    "        for i in range(fold_size):\n",
    "            classification = classify_knn(X_train, T_train, X_fold[i], K)\n",
    "            actual_class = T_fold[i][0]\n",
    "            if classification == actual_class:\n",
    "                if classification == 1:\n",
    "                    true_epi += 1\n",
    "                elif classification == 2:\n",
    "                    true_stroma += 1\n",
    "            elif classification != actual_class:\n",
    "                if classification == 1:\n",
    "                    false_epi += 1\n",
    "                elif classification == 2:\n",
    "                    false_stroma += 1\n",
    "\n",
    "    error_count = false_epi + false_stroma\n",
    "    average_error = (float(error_count) / float(fold_count)) / float(fold_size)\n",
    "    tp = float(true_epi) / float(fold_count)\n",
    "    tn = float(true_stroma) / float(fold_count)\n",
    "    fp = float(false_epi) / float(fold_count)\n",
    "    fn = float(false_stroma) / float(fold_count)\n",
    "    mcc = ((tp*tn)-(fp*fn)) / (((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))**(0.5))\n",
    "    print '---'\n",
    "    print 'K = %d' % K\n",
    "    print '0/1 loss = %f' % average_error\n",
    "    print 'MCC = %f' % (mcc)\n",
    "    print '---'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Naive Bayes classifier and use cross-validation to calculate 0/1 loss and Matthews correlation coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bayes(X, T):\n",
    "\n",
    "    def classify_bayes(X, T, x_new, parameters):\n",
    "        prob = {}\n",
    "        for cl in parameters:\n",
    "            prob[cl] = parameters[cl]['prior']\n",
    "            for i, m in enumerate(parameters[cl]['mean']):\n",
    "                vari = parameters[cl]['vars'][i]\n",
    "                prob[cl] *= 1.0 / np.sqrt(2.0 * np.pi * vari)\n",
    "                prob[cl] *= np.exp((-0.5 / vari) * (x_new[i] - m) ** 2)\n",
    "        if prob[1] > prob[2]:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "\n",
    "    true_epi = 0\n",
    "    true_stroma = 0\n",
    "    false_epi = 0\n",
    "    false_stroma = 0\n",
    "\n",
    "    for fold in range(fold_count):\n",
    "        # divide folds into validation and training data\n",
    "        lower = fold_size * fold\n",
    "        upper = fold_size * (fold + 1)\n",
    "        X_fold = X[lower:upper]\n",
    "        X_train = np.delete(X, np.arange(lower, upper, 1), 0)\n",
    "        T_fold = T[lower:upper]\n",
    "        T_train = np.delete(T, np.arange(lower, upper, 1), 0)\n",
    "\n",
    "        # calculate Gaussian parameters\n",
    "        parameters = {}\n",
    "        for cl in [1, 2]:\n",
    "            data_pos = np.where(T_train == cl)[0]\n",
    "            class_pars = {}\n",
    "            class_pars['mean'] = X_train[data_pos, :].mean(axis=0)\n",
    "            class_pars['vars'] = X_train[data_pos, :].var(axis=0)\n",
    "            class_pars['prior'] = 1.0 * len(data_pos) / len(X_train)\n",
    "            parameters[cl] = class_pars\n",
    "\n",
    "        # perform classification and quantify results\n",
    "        for i in range(fold_size):\n",
    "            classification = classify_bayes(X_train, T_train, X_fold[i], parameters)\n",
    "            actual_class = T_fold[i][0]\n",
    "            if classification == actual_class:\n",
    "                if classification == 1:\n",
    "                    true_epi += 1\n",
    "                elif classification == 2:\n",
    "                    true_stroma += 1\n",
    "            elif classification != actual_class:\n",
    "                if classification == 1:\n",
    "                    false_epi += 1\n",
    "                elif classification == 2:\n",
    "                    false_stroma += 1\n",
    "\n",
    "\n",
    "    error_count = false_epi + false_stroma\n",
    "    average_error = (float(error_count) / float(fold_count)) / float(fold_size)\n",
    "    tp = float(true_epi) / float(fold_count)\n",
    "    tn = float(true_stroma) / float(fold_count)\n",
    "    fp = float(false_epi) / float(fold_count)\n",
    "    fn = float(false_stroma) / float(fold_count)\n",
    "    mcc = ((tp*tn)-(fp*fn)) / (((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))**(0.5))\n",
    "    print '---'\n",
    "    print '0/1 loss = %f' % average_error\n",
    "    print 'MCC = %f' % (mcc)\n",
    "    return average_error\n",
    "\n",
    "bayes(X, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rank features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_errors = []\n",
    "for i in range(feature_count):\n",
    "    print i\n",
    "    feature = X[:, [i]]\n",
    "    epi_indices = np.where(T == 1)[0]\n",
    "    stroma_indices = np.where(T == 2)[0]\n",
    "    epi_greater = 0.0\n",
    "    stroma_greater = 0.0\n",
    "    for epi in epi_indices:\n",
    "        for stroma in stroma_indices:\n",
    "            if feature[epi][0] > feature[stroma][0]:\n",
    "                epi_greater += 1.0\n",
    "            elif feature[epi][0] < feature[stroma][0]:\n",
    "                stroma_greater += 1.0\n",
    "    performance = abs(((epi_greater / (epi_greater + stroma_greater)) - 0.5) * 2.0)\n",
    "    feature_errors.append((i, performance))\n",
    "feature_errors = sorted(feature_errors, key=lambda d: d[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate performance of Naive Bayes classifier using subsets of the best N features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j in range(1, feature_count + 1):\n",
    "    best_features = feature_errors[:j]\n",
    "    X_best = X[:, best_features]\n",
    "\n",
    "    print '0/1 loss for top %d features = %f' % (j, bayes(X_best, T))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
